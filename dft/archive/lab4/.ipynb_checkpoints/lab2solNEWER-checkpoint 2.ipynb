{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EE 120 Lab 2: The Fast Fourier Transform (FFT) \n",
    "v1 - Fall 2018: Dominic Carrano  \n",
    "v2 - Spring 2019: Dominic Carrano, Sid Kaushik, Shubhra Ganguly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import timeit, time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "The background section for this lab is mostly focused on how the DFT ties into the other Fourier you've learned (DTFS, DTFT) rather than the actual FFT algorithm. The algorithm is discussed within Question 2 in great detail, and we hope you find the explanation given there to be sufficient for completing the lab. If you don't, however, checkout the References section at the end, as it's filled with resources that go into even more depth on the subject.\n",
    "\n",
    "### DFT related to DTFS\n",
    "\n",
    "Living in the discrete world, we have 2 Fourier-based tools available to us: the DTFS and DTFT. Again, the DTFT of a signal is a continuous (albeit 2-pi periodic) waveform, so we can't use that. And our other option, the DTFS, relies on the assumption that the signal being analyzed is periodic. \n",
    "\n",
    "So, what we use is related to both, and is known as the **Discrete Fourier Transform (DFT)**. The DFT takes a length $N$ vector $x[n]$ (aka our time-domain signal) and returns back a length $N$ vector $X[k]$ (also a \"signal\", but in the frequency domain). On your homework, you showed that the DTFS and DFT are very intimately related - they're basically the same, but the DFT coefficients are scaled versions of the DTFS coefficients.\n",
    "\n",
    "### DFT related to DTFT\n",
    "\n",
    "The DFT (for an input of size $N$), it turns out, is actually just the DTFT sampled at $N$ uniformly spaced points for one period of the DTFT, which is one of the reasons it's so nice to use. Let's look at the analysis (the ones that take us from time into frequency) equation for each transform to gain some idea of why this is true.\n",
    "\n",
    "Assuming our discrete-time signal is $x[n]$, we have its DFT $X[k]$ and DTFT $X(\\omega)$ given by:\n",
    "\n",
    "$$X[k] = \\sum_{n = 0}^{N - 1} x[n]e^{-i\\frac{2\\pi}{N}kn}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ X(\\omega) = \\sum_{n = -\\infty}^{\\infty} x[n]e^{-i\\omega n}$$\n",
    "\n",
    "But we said our signal $x[n]$ is only of duration $N$, and we essentially disregard any other samples (in theory land, we allow for infinite-duration signals, but again, we're dealing with computers with finite memory here, so just being discrete instead of continuous is not enough - the signal must also be finite-duration). In that case we can rewrite the summation of our DTFT from $0$ to $N - 1$:\n",
    "\n",
    "$$X(\\omega) = \\sum_{n = 0}^{N - 1} x[n]e^{-i\\omega n}$$\n",
    "\n",
    "Now, we've said that the DFT is just a sampling of the DTFT, so each $k$ corresponds to a different sample. Then we're taking $N$ samples from a $2\\pi$ length interval, so each one should be spaced a distance $\\frac{2\\pi}{N}$ apart, and then $\\frac{2\\pi}{N}k$ should be the frequency that the $k$th DFT \"bin\" grabs. And indeed, if we set $\\omega = \\frac{2\\pi}{N}k$ for the DTFT equation, we now get:\n",
    "\n",
    "$$X\\bigg(\\frac{2\\pi}{N}k\\bigg) = \\sum_{n = 0}^{N - 1} x[n]e^{-i\\frac{2\\pi}{N}k n}$$\n",
    "\n",
    "This is our DTFT parameterized in terms of $\\frac{2\\pi}{N}k$ (remember that $N$ is some fixed constant - the variable here is $k$). And since we can only access samples at these intervals, we re-index the left hand side to be purely in terms of k (so it's now discretized, and we'll switch back from parentheses to square brackets in terms of notation), giving us the DFT:\n",
    "\n",
    "$$X[k] = \\sum_{n = 0}^{N - 1} x[n]e^{-i\\frac{2\\pi}{N}k n}$$\n",
    "\n",
    "This is the best way to think about the DFT: it's a uniform sampling of one period of the DTFT. From this, various properties become obvious. For example, the longer a DFT we take (by padding the signal with zeros), the better approximation of the DTFT we get."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\mathcal{Q}$1a: Naive DFT Code\n",
    "\n",
    "We'll begin by implementing the DFT the \"naive\" way by simply translating the DFT analysis equation into code. Recall that if we have a length $N$ vector of data (or, in a computer, a numpy array of length $N$), the DFT analysis equation returns a sampling of $N$ points of one-period of its DTFT according to:\n",
    "\n",
    "$$X[k] = \\sum_{n = 0}^{N-1} x[n] e^{-i\\frac{2\\pi}{N}nk}$$\n",
    "\n",
    "for $k, n \\in \\{0, 1, 2, ..., N - 1\\}$. In general, $X[k]$ is a complex-valued array, even if $x[n]$ is purely real, which we must account for when writing our code by declaring our output type as `np.complex128` (a pair of floating-point \"real\" numbers, each 64 bits). If we don't, numpy will throw out the imaginary part when doing our computations, and return the incorrect result.\n",
    "\n",
    "Fill in the function `dft` below to compute the DFT based on the analysis equation.\n",
    "\n",
    "**Optional:** Using Python's numpy capabilities, you can vectorize your code to make it faster by a constant factor. It will still be  For more information refer to the 6th reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dft(x):\n",
    "    N = len(x)\n",
    "    X = np.zeros(N, dtype=np.complex128) # output array\n",
    "    ## YOUR CODE HERE ##\n",
    "    for k in range(N):\n",
    "        for n in range(N):\n",
    "            X[k] += x[n] * np.exp(-1j * 2 * np.pi * n * k / N)   \n",
    "    ## YOUR CODE HERE ##\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a couple quick sanity checks on our naive DFT function to make sure it returns the correct results. Run the cell below to check your work. If all tests do not print \"True\" for whether or not they passed, your `dft` function is buggy. \n",
    "\n",
    "Don't worry if it takes around half a minute (it shouldn't take more than 3-4 minutes to run all 5 of the tests) for the fourth and fifth tests to run - it's because we wrote a naive implementation of the DFT, which takes a long time for large inputs! By the end of this notebook, we'll have made big improvements, and hopefully these tests will make you appreciate the FFT a bit, as 1024 and 2048 are fairly commonly used data chunk sizes to take FFTs of in the real world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_complex_array(N):\n",
    "    \"\"\"\n",
    "    Generates a length N numpy array of complex numbers whose real and imaginary \n",
    "    parts are both chosen uniformly at random from the interval [0, 1).\n",
    "    \"\"\"\n",
    "    re = np.random.rand(N)\n",
    "    im = np.random.rand(N)\n",
    "    return re + 1j*im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_fft_tests(my_fft, radix):\n",
    "    \"\"\"\n",
    "    Run tests comparing FFT functions f, g, and display information about\n",
    "    pass/fail and runtime. This is not meant to be used for performance\n",
    "    profiling, rather just a ballpark for whether or not the function's\n",
    "    runtimes are reasonable, as the time will also include printing time\n",
    "    which technically shouldn't be factored in.\n",
    "    \"\"\"\n",
    "    ref_fft = np.fft.fft\n",
    "    t0 = time.time()\n",
    "    \n",
    "    # three tests on fixed data\n",
    "    x1 = np.array([1, 2, 3])\n",
    "    print(\"Test 1 passed: {0}\".format(np.allclose(my_fft(x1), ref_fft(x1))))\n",
    "\n",
    "    x2 = np.array([-1, 1j, -1, 1j])\n",
    "    print(\"Test 2 passed: {0}\".format(np.allclose(my_fft(x2), ref_fft(x2))))\n",
    "\n",
    "    x3 = np.zeros(radix ** 8)\n",
    "    print(\"Test 3 passed: {0}\".format(np.allclose(my_fft(x3), ref_fft(x3))))\n",
    "\n",
    "    # randomized larger datasets to test on\n",
    "    x4 = np.random.rand(radix ** 12)\n",
    "    print(\"Test 4 passed: {0}\".format(np.allclose(my_fft(x4), ref_fft(x4))))\n",
    "\n",
    "    x5 = random_complex_array(radix ** 12)\n",
    "    print(\"Test 5 passed: {0}\".format(np.allclose(my_fft(x5), ref_fft(x5))))\n",
    "    \n",
    "    tf = time.time()\n",
    "    print(\"Tests took {0} secs\".format(round(tf - t0, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_fft_tests(dft, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\mathcal{Q}$1b: Naive DFT (Analysis)\n",
    "\n",
    "Now that we've written our naive DFT function, let's do some basic performance analysis on it. Answer the questions below. At the end of the lab, we'll actually plot the runtime as a function of the input size, comparing our functions against the state of the art, but that will be fairly computationally intensive, so we'll just do the \"pen and paper\" analysis for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: How many complex multipications are performed by `dft`?**  \n",
    "<span style=\"color:blue\">A: (Your Answer Here!) $N$, one per term in the summation, when multiplying $x[n]$ by $e^{-i\\frac{2\\pi}{N}kn}$.</span>\n",
    "\n",
    "**Q: How many complex additions are performed by `dft`?**   \n",
    "<span style=\"color:blue\">A: (Your Answer Here!) $N-1$, one per term in the summation minus one for the fact that we add the first two terms with a single addition operation. However, in the solution given here, we begin by zeroing out our array and then adding up all the terms, so technically $N$ complex additions are performed by the given solution code for `dft`. You could achieve $N-1$ instead by doing the first multiplication and then writing that to the array to start rather than explicitly add it to zero. For a vectorized approach to get a performance boost (CS 61C anyone?), create a matrix where each column (or row) holds the multiplication results, and then sum the columns (or rows) to obtain your coefficients. Either answer is acceptable, as long as it's consistent with your implementation. Ultimately, the extra operation doesn't make any noteworthy difference for large enough $N$, which is all we really care about with asymptotic analysis.</span>\n",
    "\n",
    "**Q: How many total complex number operations are performed by `dft`?**:  \n",
    "<span style=\"color:blue\">A: (Your Answer Here!) $N\\cdot(N-1)$ or $N\\cdot N$ depending on your implementation, as we do all $N$ of the multiplications once per term we compute. This gives a runtime of $O(N^2)$.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\mathcal{Q}$2: The Fast Fourier Transform\n",
    "\n",
    "### Background\n",
    "\n",
    "Now that we've implemented the DFT the slow way, we'll do it the faster way: the Fast Fourier Transform, or FFT for short. \n",
    "\n",
    "#### Decimation In Time vs. Decimation In Frequency\n",
    "\n",
    "There are two main classes of FFT algorithms: decimation in time (DIT), and decimation in frequency (DIF). They both use the same high-level idea of exploiting symmetries in computation to reduce the number of stages of computation from $O(N)$ (i.e. one per term) to $O(\\log_2(N))$. The *decimation in time* algorithm does so by decomposing $x[n]$ into successively smaller subsequences. The *decimation in frequency* algorithm decomposes $X[k]$ into successively smaller subsequences. In this lab, we'll implement the decimation in *time* algorithm. \n",
    "\n",
    "#### Zero-Padding\n",
    "\n",
    "This algorithm assumes that $N$, the length of $x[n]$, is a power of 2, and exploits symmetry in computations with an eye toward decomposing things in terms of powers of two.\n",
    "\n",
    "If the input length is not a power of two, there are two ways to handle it:\n",
    "1. Just zero pad to the nearest power of two, and apply the algorithm. Zero-padding the input just results in a finer sampling of the DTFT (higher $N$ implies more points that you sample of the $2\\pi$ interval, since the DFT just samples the DTFT at $N$ uniformly spaced points), so it's not a big deal in terms of functionality. In fact, it's sometimes also done to get cleaner results when the DFT is used for spectrum analysis, even if computation time isn't a concern.\n",
    "2. Call `dft`, your naive DFT routine, so that you get a DFT of the exact requested length, at the cost of more computation.\n",
    "\n",
    "We'll take the second approach here - if the input length is not a power of two, just return `dft(x)`, so we get the exact length requested. This gives anyone who uses our FFT functions more fine-grained control - if they want to zero pad, they can do it themselves.\n",
    "\n",
    "*(Note for the interested reader: There's actually a third way to still use a radix-2 FFT on a non-power of two size input without zero-padding to sacrifice getting the exact result or using a naive DFT; this is the \"Chirp-Z transform\", check out the 4th reference at the end.)*\n",
    "\n",
    "#### Meaning of \"Radix-2\"\n",
    "\n",
    "This assumption about input length being a power of two is why this specific FFT implemetation is referred to as \"radix-2\". You can also implement a version for higher radices that will then be suited to signal lengths divisible by powers of those radices. Many modern FFT software libraries use a \"mixed-radix\" FFT that combines radix-2, radix-3, radix-5 (etc.) helper functions into one master FFT function, and it will allow you to compute an FFT for any signal length that can be factored into powers of 2, 3, 5, 7. Given every positive integer above 1 can be factored into a product of powers of primes uniquely (Fundamental Theorem of Arithmetic), if we had a radix-$p$ FFT function for every prime number $p$, we could just have them call each other to solve the various sub-FFTs (as you'll see in the next section, the FFT works by decomposing a DFT into a bunch of smaller DFTs - this is what's meant by \"sub-FFTs\") and we'd never have to worry about not being able to use the FFT due to input length. \n",
    "\n",
    "In this lab, we'll only do radix 2 and 3. In many cases, your input sizes are at most around the order of 10000-100000 samples/data points.<span style=\"color:red\"><sup>****</sup></span>\n",
    "\n",
    "\n",
    "<span style=\"color:red\"><sup>****</sup></span>What about having more than ~10000 data points? In practice, most data you're taking FFTs of is sampled from some continuous phenomenon (voice recording, radio waves, etc.), and more than a few tens of thousands of samples tends to correspond to a fairly long range of time. In this long period of time, it's very rare to have constant frequency components, or what's known as a \"stationary signal\" (example: you're recording noises in the park, and a bird chirps near your microphone - your frequencies will be vastly different during that chirp!), and so it's inappropriate to just apply one single FFT to the entire dataset, as you're missing the whole picture. Instead, people use the \"Short-Time Fourier Transform\" (STFT), (which isn't actually a new transform, rather it's just the DFT/FFT applied to a signal after splitting it into chunks) by breaking the data into blocks of size 512 or 1024, taking FFTs of those, and analyzing how the frequency content looks for each chunk, which corresponds to some time range; the output here is no longer a 1d frequency vector, rather a 2d \"spectrogram\" with time on the x-axis and FFT/frequency components on the y-axis. For more, take EE 123!\n",
    "\n",
    "### The Algorithm\n",
    "\n",
    "The core concept of the Radix-2 DIT algorithm is the rearranging of the DFT of the signal $x[n]$ into two parts: a sum over the even-numbered indices $n={2m}$ and a sum over the odd-numbered indices $n={2m+1}$. Starting with the analysis equation,\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    X[k] &= \\sum_{n = 0}^{N-1} x[n] e^{-i\\frac{2\\pi}{N}nk} \\\\\n",
    "    &= \\sum_{m = 0}^{N/2-1} x[2m] e^{-i\\frac{2\\pi}{N}(2m)k} + \\sum_{m = 0}^{N/2-1} x[2m + 1] e^{-i\\frac{2\\pi}{N}(2m + 1)k} \\\\\n",
    "    &= \\sum_{m = 0}^{N/2-1} x[2m] e^{-i\\frac{2\\pi}{N/2}mk} + e^{-i\\frac{2\\pi}{N}k}\\sum_{m = 0}^{N/2-1} x[2m + 1] e^{-i\\frac{2\\pi}{N/2}mk} \\\\\n",
    "    &= E[k] + e^{-i\\frac{2\\pi}{N}k} O[k]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $E[k]$ is the $N/2$ length DFT of the signal $e[n] = [x[0], x[2], x[4], ..., x[N / 2]]$ and $O[k]$ the $N/2$ length DFT of the signal $o[n] = [x[1], x[3], x[5], ..., x[N / 2 - 1]]$. \n",
    "\n",
    "We've split the single Discrete Fourier transform into two terms which themselves look very similar to smaller Discrete Fourier Transforms, one on the odd-numbered values, and one on the even-numbered values. We haven't saved any computational cycles just yet. Each term requires(N/2)∗N computations which totals to N2.\n",
    "\n",
    "The trick comes in making use of symmetries in each of these terms. Because the range of k is 0≤k<N, while the range of n is 0≤n<M≡N/2, we see from the symmetry properties above that we need only perform half the computations for each sub-problem. Our \n",
    "O[N2] computation has become O[M2], with M half the size of N.\n",
    "\n",
    "As long as our smaller Fourier transforms have an even-valued M, we can reapply this divide-and-conquer approach, halving the computational cost each time, until our arrays are small enough that the strategy is no longer beneficial. In the asymptotic limit, this recursive approach scales as O[NlogN].  \n",
    "\n",
    "This is often visualized through the following \"butterfly diagram\", shown here for $N=8$, named for the way that the multiplications at the end are rectified with lines crossing over each other like butterfly wings:\n",
    "\n",
    "<br>  \n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/c/cb/DIT-FFT-butterfly.png\" width=\"600\">\n",
    "\n",
    "<br>\n",
    "\n",
    "Most literature on the FFT uses the shorthand notation $W_N = e^{-i\\frac{2\\pi}{N}}$, so that $W_N^k = e^{-i\\frac{2\\pi}{N}k}$, which we see in the diagram - this is just the term in front of $O[k]$ from our algebra.\n",
    "\n",
    "**So, in summary, the radix-2 DIT FFT algorithm is:**\n",
    "1. Split $x[n]$ into the even and odd indexed components, $e[n]$ and $o[n]$.\n",
    "2. Take the $N/2$ point FFTs (recursively) of these, call then $E[k]$ and $O[k]$.\n",
    "3. Compute $X[0], ..., X[N/2 - 1]$ via $X[k] = E[k] + e^{-i\\frac{2\\pi}{N}k} O[k]$. Note that $E[k], O[k]$ are both $N/2$ periodic, so all that actually changes between computing the first and second half of $X[k]$ is the weight applied to $O[k]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\mathcal{Q}$2a: Radix-2 DIT FFT (Code)\n",
    "\n",
    "Implement the radix-2 Decimation in Time FFT algorithm in the function `radix2_dit_fft`. If the length of the input array `x` is not a power of two, just call `dft` on it. You may use the provided `is_power_of_two` function, if you find it helpful, although you certainly are not required to use it or understand how it works. You should not need more than 10-15 lines of code.\n",
    "\n",
    "Hint: The FFT is a recursive algorithm. Think about what your base case should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_power_of_two(n):\n",
    "    \"\"\"Return if n is a power of 2, assuming n is a non-negative integer.\"\"\"\n",
    "    return n & (n - 1) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def radix2_dit_fft(x):\n",
    "    \"\"\"\n",
    "    Radix-2 decimation in time FFT algorithm.\n",
    "    \"\"\"\n",
    "    N = len(x)\n",
    "    X = np.zeros(N, dtype=np.complex128)\n",
    "    \n",
    "    ## YOUR CODE HERE ##\n",
    "    if not is_power_of_two(N):   # input size not a power of two, use dft\n",
    "        return dft(x)\n",
    "    if N == 1:                   # base case - DFT of a single value is that value\n",
    "        return x\n",
    "    even, odd = x[::2], x[1::2]  # even/odd split + size N/2 FFTs\n",
    "    E, O = radix2_dit_fft(even), radix2_dit_fft(odd)\n",
    "    for k in range(N):\n",
    "        X[k] = E[k % (N // 2)] + np.exp(-2 * np.pi * 1j * k / N) * O[k % (N // 2)]\n",
    "    ## YOUR CODE HERE ##\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Note: This is definitely not the fastest radix-2 DIT FFT implementation possible, you could certainly come up with one that's faster by a nontrivially large constant factor by accounting for the information given in the second part of step (3) of the algorithm description. The code here has been optimized for clarity, not necessarily speed (although it does achieve the desired $O(N \\log N)$ runtime asymptotically).</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the tests to verify correctness. If you implemented `radix2_dit_fft` correctly, all 5 tests should be passing, and should complete **in less than 1 second**. For reference, the staff solution, when run on a 2015 MacBook Pro, takes around .2 seconds for all tests to complete, although you don't have to worry about speed as long as it completes in under a second. If both of these aren't true, your implementation isn't correct. Things to check:\n",
    "- *(Functionality)* Remember, $E[k]$ and $O[k]$ only have $N/2$ elements, so you need to account for the reuse of their terms/wraparound somehow. There are many different approaches.\n",
    "- *(Functionality)* Just like in `dft`, make sure whatever array you use for the output, $X[k]$, is initialized to be of type `complex128` so you don't discard imaginary parts!\n",
    "- *(Speed)* Make sure your condition for checking if the length of x is a power of 2 is correct; you may be incorrectly calling `dft` when you don't need to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_fft_tests(radix2_dit_fft, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\mathcal{Q}$2b: Radix-2 DIT FFT (Analysis)\n",
    "\n",
    "**Q: How many stages of computation are there in the radix-2 FFT algorithm?**  \n",
    "<span style=\"color:blue\">A: (Your Answer Here!) $\\log_2(N)$, as we successively break the size $N$ input into two $N/2$ DFTs, which then each turn into two more $N/4$ DFTs, then $N/8$, and so on, with a term once per time $N$ can be divided by two. In terms of Big-O notation, the number of computation stages (alternatively: the depth of the recursive tree of calls to our FFT function) is $O(\\log(N))$. </span>\n",
    "\n",
    "**Q: How many complex multiplications are there in each stage of the radix-2 FFT algorithm (across all recursive calls at a single level)?**  \n",
    "<span style=\"color:blue\">A: (Your Answer Here!) $N$ for the $e^{-i\\frac{2\\pi}{N}k} O[k]$ term, which we do $N$ of, one per value of $k$. However, a more optimized version would account for the $N/2$ periodicity to reduce this to $N/2$ (check out slides 24-27 of reference 3's second link).</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\mathcal{Q}$2c: Radix-3 DIT FFT\n",
    "\n",
    "Now that you've implemented radix-2 DIT, let's do radix-3! We'll have to re-derive the algorithm here, using the radix-2 case as an inspiration, and then we'll translate it to code. After you do this for two different radices, you'll have a pretty solid understanding of the idea behind decimation in time algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the radix 2 case, we'll start here by assuming the length of our input is a power of 3. If not, we'll just use `dft` as before. Instead of splitting our data into even and odd indexed components, we'll now split it into indices that are divisible by 3 (0, 3, 6, 9, ...), indices that have a remainder of 1 when divided by 3 (1, 4, 7, 10, ...) and indices that have a remainder of 2 when divided by 3 (2, 5, 8, 11, ...). Instead of $e[n], o[n]$ and $E[k], O[k]$, we'll call them $f[n], s[n], t[n]$ and $F[k], S[k], T[k]$ for first, second, and third. Just as $N/2$ meant floor division in the radix 2 case, $N/3$ means floor division here. \n",
    "\n",
    "Starting with the DFT analysis equation, we have:  \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    X[k] &= \\sum_{n = 0}^{N-1} x[n] e^{-i\\frac{2\\pi}{N}nk} \\\\\n",
    "    &= \\sum_{m = 0}^{N/3-1} x[3m] e^{-i\\frac{2\\pi}{N}(3m)k} + \\sum_{m = 0}^{N/3-1} x[3m + 1] e^{-i\\frac{2\\pi}{N}(3m + 1)k} + \\sum_{m = 0}^{N/3-1} x[3m + 2] e^{-i\\frac{2\\pi}{N}(3m + 2)k} \\\\\n",
    "    &= \\sum_{m = 0}^{N/3-1} f[n] e^{-i\\frac{2\\pi}{N}(3m)k} + \\sum_{m = 0}^{N/3-1} s[n] e^{-i\\frac{2\\pi}{N}(3m + 1)k} + \\sum_{m = 0}^{N/3-1} t[n] e^{-i\\frac{2\\pi}{N}(3m + 2)k} \\\\\n",
    "    &= \\sum_{m = 0}^{N/3-1} f[n] e^{-i\\frac{2\\pi}{N/3}mk} + e^{-i\\frac{2\\pi}{N}k}\\sum_{m = 0}^{N/3-1} s[n] e^{-i\\frac{2\\pi}{N/3}mk} + e^{-i\\frac{2\\pi}{N}k}\\sum_{m = 0}^{N/2-1} t[n] e^{-i\\frac{2\\pi}{N/3}mk} \\\\\n",
    "    &= F[k] + e^{-i\\frac{2\\pi}{N}k}S[k] + e^{-i\\frac{2\\pi}{N}2k}T[k]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $F[k], S[k], T[k]$ are $N/3$-length DFTs of their respective signals. Just as we broke the problem of a length $N$ DFT into two length $N/2$ DFTs for radix-2, we've now broken the problem of a length $N$ DFT into three length $N/3$ DFTs for radix-3! Hopefully you can see how we could generalize to any arbitrary radix from here. \n",
    "\n",
    "**More generally: This idea of splitting the length $N$ DFT summation into $k$ size $N/k$ summations is all there is to a radix-$k$ decimation in time FFT. The reduction in computation comes from the fact that we have $\\log_k(N)$ stages of computation, each taking $O(N)$ operations. The naive DFT algorithm has $N$ \"stages\" (one per DFT coefficient) that each take $O(N)$ operations.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've derived the algorithm, implement it in the function `radix3_dit_fft`! Again, you may use the function `is_power_of_three` if you find it to be useful, but you're not required to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_power_of_three(n):\n",
    "    \"\"\"Return if n is a power of 3, assuming n is an integer.\"\"\"\n",
    "    return 3 ** int(round(log(n, 3))) == n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def radix3_dit_fft(x):\n",
    "    \"\"\"\n",
    "    Radix-3 decimation in time FFT algorithm.\n",
    "    \"\"\"\n",
    "    N = len(x)\n",
    "    X = np.zeros(N, dtype=np.complex128)\n",
    "    \n",
    "    ## YOUR CODE HERE ##\n",
    "    if not is_power_of_three(N):                      # input size not a power of three, use dft\n",
    "        return dft(x)\n",
    "    if N == 1:                                        # base case - DFT of a single value is that value\n",
    "        return x\n",
    "    first, second, third = x[::3], x[1::3], x[2::3]   # split into components with indices of 0, 1, and 2 (mod 3)\n",
    "    F, S, T = radix3_dit_fft(first), radix3_dit_fft(second), radix3_dit_fft(third)\n",
    "    for k in range(N):\n",
    "        Wn = np.exp(-2 * np.pi * 1j / N)\n",
    "        X[k] = F[k % (N // 3)] + (Wn ** k) * S[k % (N // 3)] + (Wn ** (2 * k)) * T[k % (N // 3)]\n",
    "    ## YOUR CODE HERE ##\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_fft_tests(radix3_dit_fft, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we test up to size $3^{12}$ inputs in these tests, it's reasonable for them to take a few seconds ($N \\log N$ is a great improvement over $N^2$, but for large enough $N$, it will still be nontrivial), but they should still be faster than the original DFT ones. These will at least get you a sense of whether or not your implementation is correct.\n",
    "\n",
    "For reference, the staff solution on a 2015 MacBook Pro takes about 35-45 seconds to complete. As long as you're within that rough ballpark, you're probably fine. It will differ based on hardware. In Q3, you'll be able to see whether or not you're actually achieving the expected $N\\log N$, this is just a proxy for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\mathcal{Q}$3: Performance Profiling\n",
    "\n",
    "Now that we've got all our functions working, let's actually plot how well they do against each other, as well as numpy's `fft` function. We'll do this by analyzing:  \n",
    "a. Our naive DFT and radix 2 FFT against numpy's FFT, but with zero-padding to the next power of 2.  \n",
    "b. Our naive DFT and radix 3 FFT against numpy's FFT, with zero-padding to the next power of 3.  \n",
    "c. Numpy's FFT on its own, to get a peek into the state of the art in FFT implementations.\n",
    "\n",
    "Throughout this process we will stick to a few conventions to be consistent:\n",
    "1. Average each function's runtime for a signle input size over 20 trials. This is enough so that we reduce any measurement noise from busy background tasks without taking insanely long to run.\n",
    "2. Only test input sizes from 1 to 512 (with an exception on part b, where we'll do up to $3^7$). We want to crank it up high enough so that DFT is noticeably bad, and that we can tell the difference between our function and numpy's, but not so much that it takes forever to run.\n",
    "\n",
    "\n",
    "If that sounds a bit daunting, keep in mind we'll mostly reuse code we write once, just profiling different functions, and the hard work will be in actually interpreting what the plots mean. Run the code below, which we'll reuse throughout this question to explore the runtimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INPUT_SIZES = [x for x in range(1, 513)]     # 1, 2, 3, 4, ..., 512\n",
    "NUM_TRIALS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def time_execution(f, arg, num_trials):\n",
    "    \"\"\"\n",
    "    Returns the runtime of a single argument function f when called on arg,\n",
    "    averaged over num_trials trials.\n",
    "    \"\"\"\n",
    "    times = []\n",
    "    for _ in range(num_trials):\n",
    "        t0 = time.time()\n",
    "        f(arg)\n",
    "        tf = time.time()\n",
    "        times.append(tf - t0)\n",
    "    return sum(times) / len(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_power_of(radix, n):\n",
    "    \"\"\"\n",
    "    Returns the next power of radix after and including the number n.\n",
    "    Not numerically stable.\n",
    "    \n",
    "    >>> next_power_of(2, 5)     # next power of 2 after/including 5\n",
    "    8\n",
    "    >>> next_power_of(2, 16)    # next power of 2 after/including 16\n",
    "    16\n",
    "    >>> next_power_of(3, 81)    # next power of 3 after/including 81\n",
    "    81\n",
    "    >>> next_power_of(3, 28)    # next power of 3 after/including 28\n",
    "    81\n",
    "    \"\"\"\n",
    "    res = np.ceil(log(n, radix))\n",
    "    return radix ** res.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_avg_runtimes(f, input_sizes, num_trials, radix=2, zero_pad=False):\n",
    "    \"\"\"\n",
    "    Given an FFT function f and an iterable type (list, range, etc.) of input_sizes,\n",
    "    returns the average (over num_trials) trials runtime of f for randomly generated \n",
    "    data of each size within input_sizes.\n",
    "    \"\"\"\n",
    "    runtimes = []\n",
    "    for n in input_sizes:\n",
    "        data = random_complex_array(n)\n",
    "        if zero_pad:\n",
    "            L = next_power_of(radix, len(data)) - len(data)\n",
    "            data = np.concatenate((data, np.zeros(L)))\n",
    "        runtimes.append(time_execution(f, data, num_trials))\n",
    "    return runtimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_runtimes(input_sizes, dft_avg_times, my_avg_times, numpy_avg_times, plot_title, ymax=None):\n",
    "    plt.figure(figsize=(16, 4))\n",
    "    plt.title(plot_title)\n",
    "    plt.plot(input_sizes, dft_avg_times)\n",
    "    plt.plot(input_sizes, my_avg_times)\n",
    "    plt.plot(input_sizes, numpy_avg_times)\n",
    "    plt.ylabel(\"Average Runtime (seconds)\")\n",
    "    plt.xlabel(\"Input Size\")\n",
    "    plt.legend((\"Naive DFT\", \"Our FFT\", \"Numpy's FFT\"))\n",
    "    plt.xlim([1, input_sizes[-1]])\n",
    "    if ymax:\n",
    "        plt.ylim([0, ymax])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_runtimes2(input_sizes, dft_avg_times, my_avg_times, plot_title, ymax=None):\n",
    "    plt.figure(figsize=(16, 4))\n",
    "    plt.title(plot_title)\n",
    "    plt.plot(input_sizes, dft_avg_times)\n",
    "    plt.plot(input_sizes, my_avg_times)\n",
    "    plt.ylabel(\"Average Runtime (seconds)\")\n",
    "    plt.xlabel(\"Input Size\")\n",
    "    plt.legend((\"Naive DFT\", \"Our FFT\"))\n",
    "    plt.xlim([1, input_sizes[-1]])\n",
    "    if ymax:\n",
    "        plt.ylim([0, ymax])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_runtimes3(input_sizes, avg_times, plot_title, ymax=None):\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    plt.title(plot_title)\n",
    "    plt.plot(input_sizes, avg_times)\n",
    "    plt.ylabel(\"Average Runtime (seconds)\")\n",
    "    plt.xlabel(\"Input Size\")\n",
    "    plt.xlim([1, input_sizes[-1]])\n",
    "    if ymax:\n",
    "        plt.ylim([0, ymax])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get started on the individual analyses, note that by far the most computationally expensive thing we'll have to do is generate the average runtimes for `dft`. Since we only need to do that once, let's do it here, and store it in a variable `dft_runtimes` so that we can reuse it later. \n",
    "\n",
    "**This will take a while to run.** Feel free to add some code to your `get_avg_runtimes` function to print out the current input size it's on everytime it hits a new multiple of 50 to track progress, although you don't have to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dft_avg_times = get_avg_runtimes(dft, INPUT_SIZES, NUM_TRIALS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\mathcal{Q}$3a: Radix 2 Profiling with Zero-Padding\n",
    "\n",
    "Fill in the cells below to profile your radix-2 function for power of 2 sized inputs. Use the predefined variables  `INPUT_SIZES` and `NUM_TRIALS` to be consistent with our adopted benchmarking conventions.\n",
    "\n",
    "Call the results of your FFT and numpy's `my_times_3a` and `numpy_times_3a`, respectively.\n",
    "\n",
    "Hint: Find `my_times_3a` and `numpy_times_3a` using implemented functions. You only need two lines of code, which will both call the same function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ## \n",
    "my_times_3a = get_avg_runtimes(radix2_dit_fft, INPUT_SIZES, NUM_TRIALS, radix=2, zero_pad=True)\n",
    "numpy_times_3a = get_avg_runtimes(np.fft.fft, INPUT_SIZES, NUM_TRIALS, radix=2, zero_pad=True)\n",
    "## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the cell below to plot your results. We'll generate two separate plots for this part, and you'll need to interpret the results in both cases by filling in answers to the questions below the plots.\n",
    "\n",
    "### Plot 1: Fully Zoomed Out View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "title_3a1 = \"Runtimes for Radix-2 FFT For Zero-Padded Inputs (Zoomed Out)\"\n",
    "plot_runtimes(INPUT_SIZES, dft_avg_times, my_times_3a, numpy_times_3a, title_3a1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: Our FFT function just calls `dft` when the input isn't a power of two, yet we still seem to do a lot better than it regardless of the input size. Why?**\n",
    "\n",
    "<span style=\"color:blue\">A: (Your Answer Here!) Because of zero padding, we always guarantee ourselves \"best case\" input - the length will always be a power of two!</span></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot 2: Zoomed In View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "title_3a2 = \"Runtimes for Radix-2 FFT For Zero-Padded Inputs (Zoomed In 20x on y-axis)\"\n",
    "plot_runtimes(INPUT_SIZES, dft_avg_times, my_times_3a, numpy_times_3a, title_3a2, ymax=max(dft_avg_times)/20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: You'll notice that the DFT runtimes quickly go out of the plot, whereas numpy's stay at \"zero\" (they're actually nonzero, but they're too small to be seen here because it's been ultra-optimized), and our FFT is piecewise constant (ignoring simulation noise since we don't average over a huge number of trials). Explain why our FFT's average runtime is piecewise constant here, as well as the exact input size values at which there's a transition.**\n",
    "\n",
    "<span style=\"color:blue\">A: (Your Answer Here!) Also because of zero padding, we essentially \"bin\" the runtimes into the next power of two, so that, for example, {3, 4} have the same runtime, {5, 6, 7, 8} have the same runtime, {9, 10, 11, ..., 15, 16} have the same runtime, etc. The transitions occur each time our current input size is a power of two and then increases by one, so the boundaries are at the powers of two: 1, 2, 4, 8, 16, 32, ..., 128, 256 (the actual spikes themselves are at one greater, i.e. 2, 3, 5, 9, 17, 33, etc). Since 512 is as high as we go (not including 513) we don't see any transition at the end.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\mathcal{Q}$3b: Radix 3 Profiling with Zero-Padding\n",
    "\n",
    "Now for radix 3! This time, we'll skip straight to the zoomed in view. Fill in the cell below to get the average runtimes, and then run the second one to generate the plots. Make sure to answer the question analyzing the plot as well!\n",
    "\n",
    "This is just like Q3a. Call your results `my_times_3b` and `numpy_times_3b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ## \n",
    "my_times_3b = get_avg_runtimes(radix3_dit_fft, INPUT_SIZES, NUM_TRIALS, radix=3, zero_pad=True)\n",
    "numpy_times_3b = get_avg_runtimes(np.fft.fft, INPUT_SIZES, NUM_TRIALS, radix=3, zero_pad=True)\n",
    "## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "title_3b = \"Runtimes for Radix-3 FFT For Zero-Padded Inputs (Zoomed In 20x on y-axis)\"\n",
    "plot_runtimes(INPUT_SIZES, dft_avg_times, my_times_3b, numpy_times_3b, title_3b, ymax=max(dft_avg_times)/20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: Just as in the radix 2 case, we see piecewise constant behavior. Explain why, as well as what the input sizes are at the \"transitions\" we see.**\n",
    "\n",
    "<span style=\"color:blue\">A: (Your Answer Here!) Again zero padding \"bins\" the runtimes, this time into powers of 3. The transitions occur at the powers of 3 here: 1, 3, 9, 27, 81, 243 (with actual spikes at one after: 2, 4, 10, 28, 82, 244). The next power of 3 after 243 is greater than 512, so 243 is the last \"jump\" we see.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\mathcal{Q}$3c: Profiling numpy.fft.fft\n",
    "\n",
    "Now that we've played around with our own function, we'll do numpy's. First, just to check your understanding of your own FFT implementations, answer the following question:\n",
    "\n",
    "**Q: Suppose we profiled our radix 2 function, but without zero padding. Describe what the input size versus average runtime plot would look like, supposing we overlayed the average times for the naive DFT.**\n",
    "\n",
    "<span style=\"color:blue\">A: (Your Answer Here!) The plot would look the same as that of `dft`, except at powers of two, where it would dramatically dip. Keep in mind that while the FFT is $O(N \\log N)$, that's only assuming we use powers of 2/zero pad.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since numpy's is much faster than our other functions, and it's hard to see the relevant features for small inputs, we'll crank up both the number of trials AND the range of input sizes here. We'll also use a taller plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numpy_inputs = [x for x in range(1, 1025)]\n",
    "numpy_times = get_avg_runtimes(np.fft.fft, numpy_inputs, num_trials=100, radix=2, zero_pad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "title_3b = \"Average Runtimes For numpy's FFT Implementation\"\n",
    "plot_runtimes3(numpy_inputs, numpy_times, title_3b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: Aside from being ridiculously optimized over 40+ years of development via parallelism and other advanced techniques, the main thing numpy's FFT implementation (an underlying library known as FFTW, or Fastest Fourier Transform in the West) uses is a \"mixed radix\" approach: in addition to having routines such as the ones we wrote to handle powers of 2, 3, or other numbers, it interleaves these so that, for example, an input of size 6 (6 is not a power of 2 or a power of 3) can be computed as 3 FFTs, each of size 2, utilizing both subroutines. Considering this, what would be the \"worst case\" input to numpy's FFT function (or any, for that matter)?**\n",
    "\n",
    "<span style=\"color:blue\">A: (Your Answer Here!) An array whose length is a large prime. You can't decompose it into smaller recursive subproblems.</span>\n",
    "\n",
    "**Q: What's the best case input?**\n",
    "\n",
    "<span style=\"color:blue\">A: (Your Answer Here!) An array whose length is a power of a small prime, such as 2 or 3. Even for large powers of 2, numpy's FFT (as well as ours!) is quite good. We saw in Q3b that even for size 512, we're still not too bad, and certainly much better than the naive DFT method!</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[1] Video lecture on the FFT: https://www.youtube.com/watch?v=1mVbZLHLaf0  \n",
    "[2] Wikipedia page on FFT, which has a really good explanation/derivation of the radix-2 DIT case in particular: https://en.wikipedia.org/wiki/Cooley%E2%80%93Tukey_FFT_algorithm  \n",
    "[3] Notes from MIT's version of EE 123 on the FFT, including other versions beside radix-2 DIT: https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-341-discrete-time-signal-processing-fall-2005/lecture-notes/lec19.pdf  \n",
    "[4] Lecture slides from EE 123 Spring 2018 on FFT: https://inst.eecs.berkeley.edu/~ee123/sp18/Notes/Lecture3C.pdf and https://inst.eecs.berkeley.edu/~ee123/sp18/Notes/Lecture4A.pdf   \n",
    "[5] Chirp-Z transform: https://math.stackexchange.com/questions/77118/non-power-of-2-ffts   \n",
    "[6] Vectorizing numpy code: https://hackernoon.com/speeding-up-your-code-2-vectorizing-the-loops-with-numpy-e380e939bed3  \n",
    "[7] Modern uses of DFT: http://news.mit.edu/2009/explained-fourier  \n",
    "[8] Comprehensive DFT review: https://www.allaboutcircuits.com/technical-articles/an-introduction-to-the-discrete-fourier-transform/  "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
