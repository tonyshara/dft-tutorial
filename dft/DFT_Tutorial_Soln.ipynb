{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import scipy.cluster\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "import librosa\n",
    "%matplotlib inline\n",
    "\n",
    "import IPython.display as ipd\n",
    "from scipy import signal, linalg   #use scipy.signal.hilbert to create an envelope of the audio signal\n",
    "from scipy.signal import hilbert, chirp\n",
    "from scipy.io import wavfile\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import wave\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras import Input\n",
    "from keras.engine import Model\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, TimeDistributed, Dropout, Bidirectional, GRU, BatchNormalization, Activation, LeakyReLU, \\\n",
    "    LSTM, Flatten, RepeatVector, Permute, Multiply, Conv2D, MaxPooling2D, UpSampling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding Audio with DFT - An Application for Speech Classification#\n",
    "## EECS 16ML , Fall 2021 ##\n",
    "\n",
    "Written by Tony Shara, Adi Ganapathi, Richard Shuai, Dohyun Cheon, Larry Yan\n",
    "\n",
    "anthony.shara@berkeley.edu, avganapathi@berkeley.edu, \n",
    "richardshuai@berkeley.edu, dohyuncheon@berkeley.edu, yanlarry@berkeley.edu\n",
    "\n",
    "### Table of contents ###\n",
    "* [Introduction](#introduction)\n",
    "    \n",
    "    Fourier Features\n",
    "* [Part 0: What are fourier features?](#part0)\n",
    "    \n",
    "    Voice Recognition\n",
    "* [Part 1: Collecting Data and preprocessing](#part1)\n",
    "* [Part 2: Learning time features](#part2)\n",
    "    - [PCA In the Time Domain](#part2a)\n",
    "    - [Visualizing Clusters in the Time Domain](#part2b)\n",
    "<!--     - [Testing Classifiers](#part2c) -->\n",
    "* [Part 3: Learning Fourier features](#part3)\n",
    "    - [Perform PCA on STFT representation of a Signal](#part3a)\n",
    "    - [Visualizing Clusters in the STFT Domain](#part3b)\n",
    "* [Part 4: Testing Classifiers](#part4)\n",
    "    - [Logistic Regression](#part4a)\n",
    "    - [Classification of Audio Signals with Nets](#part4b)\n",
    "* [Part 5: Testing on your Own Voice](#part5)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"introduction\"></a>\n",
    "# Introduction #\n",
    "This lab will give you a new perspective on your data through the lens of the frequency domain by means of the Fourier Transform and variants of it. The Fourier Transform is a useful mathematical tool that takes a signal and breaks it down into its constituent frequencies.  This allows us to look at time-varying data (such as an audio signal) or spatial data (such as a 2D image) by looking at the strength of each frequency that is contributed to the data point rather than just its spatial or time dependent magnitudes (i.e., the magnitude of a waveform at some time t or the brightness of a pixel in an image). You will also learn the power of using the frequency domain as an intermediate representation of time-varying and spatial data for downstream applications such as classification of raw audio signals.\n",
    "\n",
    "In this lab, we are particularly interested in audio signals. We will first cover some of the basics of the Fourier Transform and analyze it through a series of toy examples. We will then take this understanding and apply it to the much more unstructured problem of classifying spoken digits given only a raw audio signal as input. \n",
    "\n",
    "\n",
    "### Goals ###\n",
    "The goals of this lab are to:\n",
    "* Gain a practical understanding of the frequency domain representation of data through visualizations\n",
    "* Gain an intuitive understanding of the benefits of using the frequency domain for certain types of data and applications\n",
    "* Get hands on experience tackling a difficult real world problem using techniques learned in class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part0\"></a>\n",
    "# Part 0: Fourier Features in an Audio Signal #\n",
    "\n",
    "\n",
    "A raw audio signal can look like a mess, and indeed it is.  This is because the audio signal is a complex signal composed of a linear combination of cosines, each of which can have a unique amplitude and frequency. \n",
    "\n",
    "A plot of raw audio data simply tells us about the amplitude of an audio signal at a single sample in time\n",
    "\n",
    "### Encoding audio with DFT ###\n",
    "Like all signals that computers can handle, an audio signal is a series of discrete audio samples, and because every audio signal has finite duration, it is aperiodic.  Because these audio signals are both discrete and aperiodic, we can look at the constituent frequencies of the signal by taking the Discrete Fourier Transform (DFT) of the signal.  At its core, the Discrete Fourier Transform is a linear combination of scaled complex exponentials.  Since the signal is aperidic, we need take into account every point in the signal to determine how it impacts each frequency.  Since the signal is discrete, we sum over each of the scaled complex exponentials.\n",
    "\n",
    "##### Discrete Fourier Transform (DFT): $$F(\\omega) = \\sum_{n = -\\infty}^{\\infty} x(n) e^{-i\\omega n}$$\n",
    "\n",
    "Notice how when taking the DFT of a signal, the ith complex exponential is scaled by the ith component of the signal.  Since our signal can be expressed as a vector, it looks a lot like a matrix multiplication.  In fact it is!  The Vandermonds matrix is another way to compute the DFT of signal in matrix-vector form.\n",
    "\n",
    "##### Vandermonde matrix: \n",
    "\n",
    "$$W = \n",
    "\\frac{1}{\\sqrt{N}}\n",
    "\\begin{bmatrix}\n",
    "1&1&1&1& ... &1\\\\\n",
    "1&\\omega&\\omega^2&\\omega^3& ... &\\omega^{N-1} \\\\\n",
    "1&\\omega^2&\\omega^4&\\omega^6& ...& \\omega^{2(N-1)}\\\\\n",
    "1&\\omega^3&\\omega^6&\\omega^9& ... &\\omega^{3(N-1)}\\\\\n",
    ".&.&.&.&.  & .\\\\\n",
    ".&.&.&.&.  & .\\\\\n",
    ".&.&.&.&. & .\\\\\n",
    "1&\\omega^{N-1}&\\omega^{2(N-1)}&\\omega^{3(N-1)}& ... &\\omega^{(N-1)(N-1)}\n",
    "\\end{bmatrix} ,\\omega = e^{\\frac{i2\\pi}{N}}\n",
    "$$\n",
    "\n",
    "\n",
    "If $x$ is a discrete length n time varying signal, then $Wx$ is the DFT of $x$.\n",
    "The Fast Fourier Transform is an algorithm that performs $Wx$ in O(NlogN) rather than the usual O(N^2) by leveraging symmetries in the matrix $W$.\n",
    "\n",
    "### Working with audio files ###\n",
    "\n",
    "Q1: \n",
    "Play the audio file below.  Does anything stand out about pitch of the most pronounced sound as the file plays?  How might we be able to use the DFT to learn about these features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fourier Transforms of waves #\n",
    "\n",
    "When looking at waves, what are some important characteristics of these waves that make them stand out? To answer this question, lets break down the different components of a wave.\n",
    "\n",
    "<br><br>\n",
    "<img src=\"visuals/wave_visual.gif\" alt=\"Drawing\" style=\"width: 50em;\"/>\n",
    "\n",
    "### amplitude ###\n",
    "* What is the amplitude of a wave\n",
    "* What does the amplitude at a given time tell us\n",
    "\n",
    "### wavelength / frequency ###\n",
    "* What is the wavelength of a wave\n",
    "* What is the frequency of a wave\n",
    "* how are the wavelength and frequency of a wave important\n",
    "* What new imformation can we gain by looking at the frequency of the wave\n",
    "* How can we look at our frequencies? FFT!\n",
    "\n",
    "## Frequency Spectrum ##\n",
    "\n",
    "When talking about signals in the frequency domain, we generally refer to the frequency spectrum of the signal. The frequency spectrum is a plot of the magnitude of the Fourier Transform of the signal over all frequencies from -infinity to infinity, but in practice we generally cut off the domain to only the relevant frequencies. Since the fourier transform of a signal is symmetric about the y axis, we only care to look at positive frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Signals ##\n",
    "\n",
    "As we discussed above, it can be useful to talk about a signal in terms of the energy of its constituent frequencies rather than the amplitude over time.  Let's look at the most simple example of a signal: a cosine wave. The code below plots this wave. Fill in the section to compute the FFT of the signal and plot it below to visualize the frequency respons over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = np.cos(np.arange(0, 20, 0.2))\n",
    "plt.plot(sig)\n",
    "plt.show()\n",
    "\n",
    "##BEGIN CODE##\n",
    "# Remember, the frequency response is symmetric about the y axis, so we \n",
    "# really only care to look at positive frequencies (w > 0)\n",
    "fft = np.fft.fft(sig)[:50]\n",
    "fft = np.abs(fft)\n",
    "plt.plot(fft)\n",
    "plt.show()\n",
    "##END CODE##\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = 2*np.cos(np.arange(0, 20, 0.2)*2)\n",
    "plt.plot(sig)\n",
    "plt.show()\n",
    "\n",
    "##BEGIN CODE##\n",
    "fft = np.fft.fft(sig)[:50]\n",
    "fft = np.abs(fft)\n",
    "plt.plot(fft)\n",
    "plt.show()\n",
    "##END CODE##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex Signals ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos1 = np.cos(np.arange(0, 20, 0.2))\n",
    "cos2 = 2*np.cos(np.arange(0, 20, 0.2)*2)\n",
    "cos3 = 8*np.cos(np.arange(0, 20, 0.2)*4)\n",
    "sig = cos1 + cos2 + cos3\n",
    "plt.plot(sig)\n",
    "plt.show()\n",
    "\n",
    "##BEGIN CODE##\n",
    "fft = np.fft.fft(sig)[:50]\n",
    "fft = np.abs(fft)\n",
    "plt.plot(fft)\n",
    "plt.show()\n",
    "##END CODE##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice above that in our signal, we have 3 different distinct cosines contributing to the signal. Is this evident when looking at the fourier transform of the signal?  Explain.\n",
    "\n",
    "<span style=\"color:blue\">**A**: Yes; we see a spike which corresponds to each cosine.  Cosines with a larger amplitude contribute more to energy to their respective frequencies. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ## Fourier Series ##\n",
    "\n",
    "While certain signals can be messy and hard to understand in their raw form, breaking down the signal into a combination of many simpler signals could help us better understand the seemingly complex signal. It turns out we can express any wave as a linear combination of sines and cosines.  This mathematical expression is called the <b>Fourier Series</b>.\n",
    "\n",
    "$$ X[k] = \\sum_{n = 0}^{N-1} x(n) e^{-i\\frac{2\\pi}{N}nk} $$\n",
    "\n",
    "As we saw from above, each cosine contributed some energy to a frequency coresponding to the frequency of the wave!  This means that TODO: Tony\n",
    "\n",
    "                # DO WE WANT THIS ^^^^ # -->\n",
    "\n",
    "## Sound Waves ##\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hz100 = \"test-tones/100hz.wav\"\n",
    "\n",
    "fs, data = wavfile.read(hz100)\n",
    "\n",
    "t = np.arange(0,data.shape[0]/fs, 1/fs)\n",
    "\n",
    "plt.plot(t, data)\n",
    "plt.show()\n",
    "\n",
    "ipd.Audio(\"test-tones/100hz.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = np.fft.fft(data)\n",
    "\n",
    "#magnitude spectrum\n",
    "plt.plot(np.abs(F[:11025]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex Sound Wave ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(\"SpiritInTheSky.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode SpiritInTheSky.wav with DFT and plot the frequency response of the signal.  Then compare it to the plot of the original signal of SpiritInTheSky.wav without DFT encoding. (Hint: np.fft.fft might be usefull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fs is the sampling rate of the wav file in frames per second\n",
    "# data is the audio sample\n",
    "fs, data = wavfile.read(\"SpiritInTheSky.wav\")\n",
    "data = data[:,0]\n",
    "\n",
    "# TODO: Encode SpiritInTheSky.wav with DFT and plot the frequency response of the signal\n",
    "F = np.fft.fft(data)\n",
    "F = F[:int(F.shape[0]/2)]      #only look at first half of data since it is symmetric\n",
    "F = np.abs(F)\n",
    "plt.plot(F)\n",
    "# plt.magnitude_spectrum(data, Fs=fs)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(data)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Classification #\n",
    "\n",
    "In the previous part we looked at audio files with and without DFT encodings and saw that the DFT encoding allows you to learn about the energy of different frequencies throughout the sound.  Now that we have a way to look at the different frequencies in a controlled setting, let's try to learn about the frequencies in a person's voice! We can then train a machine learning model which uses these frequencies to classify what they are saying!\n",
    "\n",
    "In this part, we will use the [free spoken digits dataset](https://github.com/Jakobovski/free-spoken-digit-dataset) open source dataset to train a variety of classification algorithms to recognize digits in human speech.  This dataset contains 50 recordings of spoken digits (0 - 9) from 6 different speakers.  Your job is to classify spoken digits in an audio signal.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part1\"></a>\n",
    "# Part 1: Collecting Data and Preprocessing #\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecting and preprocessing data is perhaps the most important and difficult part of being a machine learning or data science practitioner. Before we begin applying algorithms, we need to make sure our data is in a structured and interpretable format so that we can easily repurpose it down the road. Before we get started, let's look at a few of our labels.  Each label in our dataset represents a unique number between 0 and 9.  We will use this data to recognize a unique word, in this case, a number. Each row of our data matrix will consist of the time domain representation of a single spoken digit. Remember, we must make sure that the sampling rate of each data point is the same to ensure the correct dimensionality and consistency. It turns out that the sampling rate required to fully encapsulate a continuous signal is closely tied to the frequencies present in the raw signal. See the supporting note for more details on this. Luckily, the dataset provided has already done this, but when we collect our own test data later, we will have to do this manually.\n",
    "\n",
    "### Dataset Organization and Train/Test Split ###\n",
    "\n",
    "In order to eventually classify our data, we need to properly organize our dataset so that we can simply plug and play with different classifiers. This includes creating a train/test split of our data so that we can train on a portion of the data and test on the remaining portion. In the most realistic setting, we won't have training data on the speaker we encounter at test time, so let's limit our training data to only 5 of the speakers and leave out one speaker for the test set. We will also take the time here to compute all the different representations of our data that we will end up using later in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill in the code split he \n",
    "\n",
    "\"\"\"\n",
    "train_X: training set\n",
    "test_X: test set\n",
    "\n",
    "train_y: training labels (1 hot encoding)\n",
    "train_y_1d: training labels\n",
    "\n",
    "test_y: test labels (1 hot encoding)\n",
    "test_y_1d: test labels\n",
    "\"\"\"\n",
    "\n",
    "DATA_DIR = \"recordings/\"\n",
    "\n",
    "#Feel free to change the test speaker for each round of training/testing so that we can perform k-fold validation\n",
    "test_speaker = 'theo' \n",
    "train_X = []\n",
    "train_spectrograms = []\n",
    "\n",
    "train_y = []\n",
    "\n",
    "test_X = []\n",
    "test_spectrograms = []\n",
    "\n",
    "test_y = []\n",
    "\n",
    "pad1d = lambda a, i: a[0: i] if a.shape[0] > i else np.hstack((a, np.zeros(i - a.shape[0])))\n",
    "pad2d = lambda a, i: a[:, 0: i] if a.shape[1] > i else np.hstack((a, np.zeros((a.shape[0],i - a.shape[1]))))\n",
    "\n",
    "for fname in os.listdir(DATA_DIR):\n",
    "    try:\n",
    "        if '.wav' not in fname or 'dima' in fname:\n",
    "            continue\n",
    "        struct = fname.split('_')\n",
    "        digit = struct[0]\n",
    "        speaker = struct[1]\n",
    "        wav, sr = librosa.load(DATA_DIR + fname)\n",
    "        padded_x = pad1d(wav, 30000)\n",
    "        spectrogram = np.abs(librosa.stft(wav))\n",
    "        padded_spectogram = pad2d(spectrogram,40)\n",
    "        # Seperate your data into a train and test set.  \n",
    "        # The test set should only include data sample collected from test_speaker.\n",
    "        # The training set should include the rest of the data.\n",
    "        ###BEGIN CODE\n",
    "        if speaker == test_speaker:\n",
    "            test_X.append(padded_x)\n",
    "            test_spectrograms.append(padded_spectogram)\n",
    "            test_y.append(digit)\n",
    "        else:\n",
    "            train_X.append(padded_x)\n",
    "            train_spectrograms.append(padded_spectogram)\n",
    "            train_y.append(digit)\n",
    "        ###END CODE###\n",
    "    except Exception as e:\n",
    "        print(fname, e)\n",
    "        raise\n",
    "\n",
    "train_X = np.vstack(train_X)\n",
    "#De-mean your train set\n",
    "###BEGIN CODE###\n",
    "train_X = (train_X - np.mean(train_X))/np.std(train_X)\n",
    "###END CODE###\n",
    "train_spectrograms = np.array(train_spectrograms)\n",
    "\n",
    "train_y = to_categorical(np.array(train_y))\n",
    "train_y_1d = np.zeros(train_y.shape[0])\n",
    "for i in range(train_y.shape[0]):\n",
    "    val = np.where(train_y[i] == 1)\n",
    "    train_y_1d[i] = val[0][0]\n",
    "    \n",
    "    \n",
    "test_X = np.vstack(test_X)\n",
    "#De-mean your test set\n",
    "###BEGIN CODE###\n",
    "test_X = (test_X - np.mean(test_X))/np.std(test_X)\n",
    "###END CODE###\n",
    "test_spectrograms = np.array(test_spectrograms)\n",
    "\n",
    "test_y = to_categorical(np.array(test_y))\n",
    "test_y_1d = np.zeros(test_y.shape[0])\n",
    "for i in range(test_y.shape[0]):\n",
    "    val = np.where(test_y[i] == 1)\n",
    "    test_y_1d[i] = val[0][0]\n",
    "\n",
    "print('train_X:', train_X.shape)\n",
    "print('train_spectrograms:', train_spectrograms.shape)\n",
    "print('train_y:', train_y.shape)\n",
    "print('train_y_1d: ', train_y_1d.shape)\n",
    "\n",
    "print('test_X:', test_X.shape)\n",
    "print('test_spectrograms:', test_spectrograms.shape)\n",
    "print('test_y:', test_y.shape)\n",
    "print('test_y_1d: ', test_y_1d.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset of only 3 distinct digits.  Feel free to play around with these labels (digits)\n",
    "digits = [2, 7, 9]\n",
    "train_X_small = []\n",
    "train_y_small = []\n",
    "for i in range(train_y_1d.shape[0]):\n",
    "    if train_y_1d[i] == digits[0] or train_y_1d[i] == digits[1] or train_y_1d[i] == digits[2] :\n",
    "        train_X_small.append(train_X[i])\n",
    "        train_y_small.append(train_y_1d[i])\n",
    "train_X_small = np.vstack(train_X_small)\n",
    "train_y_small = np.array(train_y_small)\n",
    "\n",
    "test_X_small = []\n",
    "test_y_small = []\n",
    "for i in range(test_y_1d.shape[0]):\n",
    "    if test_y_1d[i] == digits[0] or test_y_1d[i] == digits[1] or test_y_1d[i] == digits[2] :\n",
    "        test_X_small.append(test_X[i])\n",
    "        test_y_small.append(test_y_1d[i])\n",
    "test_X_small = np.vstack(test_X_small)\n",
    "test_y_small = np.array(test_y_small)\n",
    "\n",
    "train_X = train_X_small\n",
    "train_y_1d = train_y_small\n",
    "\n",
    "test_X = test_X_small\n",
    "test_y_1d = test_y_small\n",
    "\n",
    "plt.plot(train_X.T)\n",
    "plt.show()\n",
    "\n",
    "print(\"train_X: \", train_X.shape)\n",
    "print(\"train_y_1d: \", train_y_1d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enveloping data for time domain clustering\n",
    "Since we want to learn about patterns in the amplitude of the audio signal over time, we can envelope our signal.  By enveloping, we are simply tracing the magnitudes of our wave and therefore focusing our attention to the general shape of the audio signal.\n",
    "\n",
    "What is the motivation behind enveloping our time domain signal?  Why can't we do PCA on the unprocessed signal?\n",
    "TODO: ANSWER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is an example of enveloping\n",
    "duration = 1.0\n",
    "fs = 400.0\n",
    "samples = int(fs*duration)\n",
    "t = np.arange(samples) / fs\n",
    "\n",
    "signal = chirp(t, 20.0, t[-1], 100.0)\n",
    "signal *= (1.0 + 0.5 * np.sin(2.0*np.pi*3.0*t))\n",
    "\n",
    "analytic_signal = hilbert(signal)\n",
    "amplitude_envelope = np.abs(analytic_signal)\n",
    "\n",
    "plt.plot(t, signal, label='signal')\n",
    "plt.plot(t, amplitude_envelope, label='envelope')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Describe what is going on in this cell (enveloping is done here)\n",
    "\n",
    "# hilberts = []\n",
    "\n",
    "# for i in range(X.shape[0]):\n",
    "#     analytic_signal = hilbert(X[i])\n",
    "#     amplitude_envelope = np.abs(analytic_signal)\n",
    "#     hilberts.append(amplitude_envelope)\n",
    "# hilberts = np.vstack(hilberts)\n",
    "# processed_X = hilberts\n",
    "\n",
    "def envelope(X):\n",
    "    enveloped_X = []\n",
    "    print(X.shape)\n",
    "    for i in range(X.shape[0]):\n",
    "        analytic_signal = hilbert(X[i])\n",
    "        amplitude_envelope = np.abs(analytic_signal)\n",
    "        enveloped_X.append(amplitude_envelope)\n",
    "    enveloped_X = np.vstack(enveloped_X)\n",
    "    print(enveloped_X.shape)\n",
    "    return enveloped_X\n",
    "\n",
    "processed_X = envelope(train_X)\n",
    "\n",
    "plt.plot(processed_X.T)\n",
    "plt.show()\n",
    "\n",
    "print(processed_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_X = processed_X[:,:23000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part2\"></a>\n",
    "# Part 2: Visualizing Our Data in Lower Dimensions  #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part2a\"></a>\n",
    "### Part 2a: PCA in the Time Domain ###\n",
    "\n",
    "We will start off by trying to visualize our data in the time domain format that it has been provided. To do this, it would be helpful to first perform PCA on the processed data matrix containing the enveloped signals. This will allow us to extract the most important dimensions of our data. We can then project our data onto these dimensions to visualize their differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA on the processed data matrix (you may use the numpy svd library).\n",
    "# Plot the eigenvalues and principical components of the new basis\n",
    "# Project the test data onto the basis\n",
    "\n",
    "###BEGIN CODE###\n",
    "U_t,S_t,Vt_t = np.linalg.svd(processed_X)\n",
    "\n",
    "plt.stem(S_t)\n",
    "plt.show()\n",
    "\n",
    "#selecting principle components (can chose variable number of principle components, but have chosen 3 for now)\n",
    "new_basis_t = np.array([Vt_t[0], Vt_t[1], Vt_t[2]]).T        \n",
    "plt.plot(new_basis_t)\n",
    "\n",
    "# Project the data onto the new basis\n",
    "proj_X_t = np.dot(processed_X, new_basis_t)\n",
    "###END CODE###\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_X_t = proj_X_t[:,:15000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part2b\"></a>\n",
    "### Part 2b: Visualizing Clusters in the Time Domain ###\n",
    "\n",
    "Here, we will write a function to visualize our test data that has been projected onto the principal component basis. We will make use of this function later when visualizing our data under different representations. Since PCA is inherently linear, it will be difficult to properly cluster 10 distinct spoken digits well given the sever non-linearity of the data. Thus, it will likely be more insightful to only visualize a subset of the data (i.e. only 2 or 3 different digits rather than all 10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we visualize our clusters by color coding the labels\n",
    "\n",
    "\n",
    "def get_PCA_clusters(X, y, labels):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    X - a matrix which is the projectin of your dataset onto your new basis\n",
    "    y - np.array contining your labels\n",
    "    labels - a list of three desired labels (strings)\n",
    "    \n",
    "    outputs:\n",
    "    label0 - an array containing points in X corresponding to labels[0]\n",
    "    label1 - an array containing points in X corresponding to labels[1]\n",
    "    label2 - an array containing points in X corresponding to labels[2]\n",
    "    \"\"\"\n",
    "    label0 = []\n",
    "    label1 = []\n",
    "    label2 = []\n",
    "\n",
    "    ###BEGIN CODE###\n",
    "    for i in range(y.shape[0]):\n",
    "        if y[i] == labels[0]:\n",
    "            label0.append(X[i])\n",
    "        if y[i] == labels[1]:\n",
    "            label1.append(X[i])\n",
    "        if y[i] == labels[2]:\n",
    "            label2.append(X[i])\n",
    "    label0 = np.vstack(label0)\n",
    "    label1 = np.vstack(label1)\n",
    "    label2 = np.vstack(label2)\n",
    "    ###END CODE###      \n",
    "    return label0, label1, label2\n",
    "    \n",
    "label0, label1, label2 = get_PCA_clusters(proj_X_t, train_y_1d, digits)    \n",
    "fig=plt.figure(figsize=(10,7))\n",
    "plt.scatter(label0[:,0], label0[:,1], c=['blue'], edgecolor='none')\n",
    "plt.scatter\n",
    "plt.scatter(label1[:,0], label1[:,1], c=['red'], edgecolor='none')\n",
    "plt.scatter\n",
    "plt.scatter(label2[:,0], label2[:,1], c=['green'], edgecolor='none')\n",
    "plt.scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A visualization of our clusters in 3D!\n",
    "\n",
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "ax.scatter3D(label0[:,0], label0[:,1], label0[:,2], c=label0[:,2], cmap='Blues');\n",
    "ax.scatter3D(label1[:,0], label1[:,1], label1[:,2], c=label1[:,2], cmap='Reds');\n",
    "ax.scatter3D(label2[:,0], label2[:,1], label2[:,2], c=label2[:,2], cmap='Greens');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part3\"></a>\n",
    "# Part 3: Learning Fourier Features #\n",
    "\n",
    "Now, we will do exactly what we did before except in the frequency domain! The idea is to see if using the frequency domain representation of our data matrix will change the way we cluster with PCA. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the FFT of the Data Matrix ##\n",
    "\n",
    "Let's start by creating a new data matrix which is the DFT of our original data matrix. Once we can do this, then the process of running PCA should be identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Perform FFT on the data matrix and plot a few of the data points in the frequency domain\n",
    "# Remember, the DFT is symmetric about the y-axis, so you only want the first half of the result of FFT\n",
    "\n",
    "###BEGIN CODE 1###\n",
    "fft_X = np.fft.fft(train_X)                     #take fft of data matrix\n",
    "fft_X = fft_X[:,:int(fft_X.shape[1]/2)]   # first half of frequeny response since symmetric\n",
    "fft_X = np.abs(fft_X)\n",
    "plt.plot(fft_X[0])\n",
    "plt.show()\n",
    "###END CODE 1###\n",
    "\n",
    "# Since certain frequencies about a certain threshold have zero or near-zero presence in the frequency response,\n",
    "# we can simply cut them off or get rid of them. This will allow us to perform PCA on a smaller matrix which will \n",
    "# save us a lot of time and prevent dead kernels. Cut off the frequencies appropriately (where the magnitudes are\n",
    "# essentially zero and then visualize the signals again.)\n",
    "\n",
    "###BEGIN CODE 2###\n",
    "fft_X = fft_X[:,:6000]\n",
    "plt.plot(fft_X[0])\n",
    "plt.show()\n",
    "###END CODE 2###\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing PCA in the Frequency Domain ##\n",
    "You guessed it. Let's now perform PCA on our new data matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now perform PCA in the frequency domain. Change the number of basis vectors, plot them, and comment\n",
    "# on the differences.\n",
    "\n",
    "###BEGIN CODE SVD###\n",
    "U_f, S_f, Vt_f = np.linalg.svd(fft_X)\n",
    "###END CODE SVD###\n",
    "\n",
    "###BEGIN CODE STEM PLOT PRINCIPAL COMPONENTS###\n",
    "plt.stem(S_f)\n",
    "plt.show()\n",
    "###END CODE STEM PLOT PRINCIPAL COMPONENTS###\n",
    "\n",
    "###BEGIN CODE CREATE PRINCIPAL COMPONENT BASIS###\n",
    "new_basis_f = np.array([Vt_f[0], Vt_f[1], Vt_f[2]]).T        # This should be the basis containing your principal components\n",
    "###END CODE CREATE PRINCIPAL COMPONENT BASIS###\n",
    "\n",
    "###BEGIN CODE PLOT BASIS VECTORS###\n",
    "plt.plot(new_basis_f)\n",
    "plt.show()\n",
    "###END CODE PLOT BASIS VECTORS###\n",
    "\n",
    "###BEGIN CODE PROJECT TEST DATA ONTO NEW BASIS###\n",
    "proj_fft_X = np.dot(fft_X, new_basis_f)\n",
    "###END CODE PROJECT TEST DATA ONTO NEW BASIS###\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Clusters in the Frequency Domain ###\n",
    "\n",
    "Call the function you created earlier to visualize the new clusters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the color coded clusters from PCA\n",
    "\n",
    "###BEGIN CODE###\n",
    "label0, label1, label2 = get_PCA_clusters(proj_fft_X, train_y_1d, digits)    \n",
    "        \n",
    "fig=plt.figure(figsize=(10,7))\n",
    "plt.scatter(label0[:,0], label0[:,1], c=['blue'], edgecolor='none')\n",
    "plt.scatter\n",
    "plt.scatter(label1[:,0], label1[:,1], c=['red'], edgecolor='none')\n",
    "plt.scatter\n",
    "plt.scatter(label2[:,0], label2[:,1], c=['green'], edgecolor='none')\n",
    "plt.scatter\n",
    "###END CODE###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this cell to visualize the clusters in a 3D scatter plot\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "ax.scatter3D(label0[:,0], label0[:,1], label0[:,2], c=label0[:,2], cmap='Blues');\n",
    "ax.scatter3D(label1[:,0], label1[:,1], label1[:,2], c=label1[:,2], cmap='Reds');\n",
    "ax.scatter3D(label2[:,0], label2[:,1], label2[:,2], c=label2[:,2], cmap='Greens');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using STFT to Encode Signal with DFT\n",
    "\n",
    "Human speech, unlike the toy audio signals we were dealing with earlier, is not static noise. In fact, individuals words themselves are dynamic and change rapidly over time which motivates some combination of a time and frequency based representation of the signal, rather than just performing a Fourier Transform on the signal and viewing it in the frequency domain. The Short Time Fourier Tranform (STFT) addresses this problem by breaking up a signal into windowed time component and computing the DFT in each of these windows of time. You will now explore the STFT of our signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "##         STFT\n",
    "##########################\n",
    "\n",
    "#Perform the STFT on our data matrix and plot the spectogram.\n",
    "# (Hint 1: Consider using the librosa library for both computing stft and plotting spectogram)\n",
    "# (Hint 2: Make sure to cut off the frequencies where there are close to 0 magnitude as we did earlier)\n",
    "# (Hint 3: The STFT returns a 2D array because there is a time and frequency component. How can we make this work\n",
    "#  with our data matrix?)\n",
    "\n",
    "###BEGIN CODE###\n",
    "stft = []\n",
    "for i in range(train_X.shape[0]):\n",
    "    temp = np.abs(librosa.stft(train_X[i]))\n",
    "    temp = temp[:400,:]\n",
    "    temp = temp.flatten()\n",
    "    stft.append(temp)\n",
    "    \n",
    "stft = np.vstack(stft)\n",
    "\n",
    "import librosa.display\n",
    "D = librosa.amplitude_to_db(np.abs(librosa.stft(train_X[0])), ref=np.max)\n",
    "librosa.display.specshow(D, y_axis='linear')\n",
    "##END CODE###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part3a\"></a>\n",
    "### Part 3a: Perform PCA on the STFT Representation of the Signal ###\n",
    "\n",
    "You know what's next! Let's perform PCA in the STFT domain and see if this gives us any better clustering. Since linearly separating 10 distinct spoken digits is quite difficult, try using a data matrix of only 2-3 digits as you did for both time domain and DFT. This will better help identify the differences between all the representations we have considered so far.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "##  PCA on STFT Signal\n",
    "############################\n",
    "\n",
    "# Perform PCA on the STFT representation of the signal and plot it. Then, as before, compute the new basis \n",
    "# (play around with different numbers of basis vectors) and plot the basis vectors. Finally, project the test \n",
    "# data onto the new basis.\n",
    "\n",
    "###BEGIN CODE###\n",
    "U_s,S_s,Vt_s = np.linalg.svd(stft)\n",
    "plt.stem(S_s)\n",
    "plt.show()\n",
    "\n",
    "new_basis_s = np.array([Vt_s[0], Vt_s[1], Vt_s[2]]).T # basis containing your principal components\n",
    "plt.plot(new_basis_s)\n",
    "plt.show()\n",
    "\n",
    "proj_stft = np.dot(stft, new_basis_s)\n",
    "###END CODE###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part3b\"></a>\n",
    "### Part 3b: Visualizing Clusters in the STFT Domain ###\n",
    "\n",
    "Call the function you created earlier to visualize the clusters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot and visualize 2D clusters from PCA\n",
    "\n",
    "###BEGIN CODE###\n",
    "label0, label1, label2 = get_PCA_clusters(proj_stft, train_y_1d, digits)    \n",
    "        \n",
    "fig=plt.figure(figsize=(10,7))\n",
    "plt.scatter(label0[:,0], label0[:,1], c=['blue'], edgecolor='none')\n",
    "plt.scatter\n",
    "plt.scatter(label1[:,0], label1[:,1], c=['red'], edgecolor='none')\n",
    "plt.scatter\n",
    "plt.scatter(label2[:,0], label2[:,1], c=['green'], edgecolor='none')\n",
    "plt.scatter\n",
    "###END CODE###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the cell to visualize 3D clusters from PCA\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "ax.scatter3D(label0[:,0], label0[:,1], label0[:,2], c=label0[:,2], cmap='Blues');\n",
    "ax.scatter3D(label1[:,0], label1[:,1], label1[:,2], c=label1[:,2], cmap='Reds');\n",
    "ax.scatter3D(label2[:,0], label2[:,1], label2[:,2], c=label2[:,2], cmap='Greens');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part4\"></a>\n",
    "# Part 4: Testing Classifiers\n",
    "\n",
    "Now that we have visualized our data in using three different representations, let's see if we can perform some classification of the spoken digits! As mentioned earlier, classifying all 10 spoken digits is a highly non-linear problem and will likely not be possible with the linear classifiers we will use at first, so make sure you are using a subset of the data matrix mentioned above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4a: Logistic Regression ##\n",
    "\n",
    "How do you expect logistic regression to perform given the PCA clusters from our dataset in (1) time domain (2) frequency domain and (3) STFT?\n",
    "\n",
    "TODO: Answer the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the shape of your processed data and then use \n",
    "# sklearn's logistic regression to classify that data\n",
    "\n",
    "###BEGIN CODE###\n",
    "print(processed_X.shape)\n",
    "clf = LogisticRegression(random_state=0).fit(processed_X, train_y_1d)\n",
    "###END CODE###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression with PCA preprocessing for dimensionality reduction\n",
    "As you should have seen in the step above, logistic regression does not converge! This is because the dataset was too large initially. This is a great place for dimensionality reduction algorithms such as PCA to shine! Let's perform PCA on our data matrix and then use logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the shape of your new data after performing PCA and then use \n",
    "# sklearn's logistic regression to classify that data\n",
    "\n",
    "###BEGIN CODE###\n",
    "print(proj_X_t.shape)\n",
    "clf = LogisticRegression(random_state=0).fit(proj_X_t, train_y_1d)\n",
    "###END CODE###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see how our classifier performs on the test set! Here, you must project the test set onto our new basis and then perform predictions. How does it do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First project the test set onto the new basis\n",
    "# Then use your classifier to get a prediction vector on your projected test set\n",
    "# Finally, compute the accuracy of your classifier on the test set\n",
    "\n",
    "###BEGIN CODE###\n",
    "processed_X_test = envelope(test_X)\n",
    "processed_X_test = processed_X_test[:,:23000]\n",
    "proj_test = np.dot(processed_X_test, new_basis_t)\n",
    "\n",
    "predictions = clf.predict(proj_test)\n",
    "error = test_y_1d - predictions\n",
    "error[np.abs(error) > 0] = 1\n",
    "print(1 - sum(error)/error.shape[0])\n",
    "###END CODE###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression on the DFT of the data matrix\n",
    "\n",
    "Now let's perform logistic regression on the frequency representation of our data matrix. As before, to ensure convergence, let's perform PCA on the DFT of the data matrix before applying logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use sklearn's logistic regression to classify your data after DFT encoding\n",
    "\n",
    "###BEGIN CODE###\n",
    "clf = LogisticRegression(random_state=0).fit(proj_fft_X, train_y_1d)\n",
    "###END CODE###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will learn a classifier on the frequency representation of our data matrix. Perform the same steps as you did for the time domain representation with the only difference being the representation of the data matrix. How does the accuracy of this classifier on the test set compare to the time domain classifier? What does this tell you about the frequency domain representation of raw audio signals?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First project the test set onto the new basis\n",
    "# Then use your classifier to get a prediction vector on your projected test set\n",
    "# Finally, compute the accuracy of your classifier on the test set\n",
    "\n",
    "###BEGIN CODE###\n",
    "test_fft = np.fft.fft(test_X)\n",
    "test_fft = test_fft[:,:int(test_fft.shape[1]/2)]   # first half of frequeny response since symmetric\n",
    "test_fft = np.abs(test_fft)\n",
    "test_fft = test_fft[:,:6000]\n",
    "proj_test = np.dot(test_fft, new_basis_f)\n",
    "\n",
    "predictions = clf.predict(proj_test)\n",
    "error = test_y_1d - predictions\n",
    "error[np.abs(error) > 0] = 1\n",
    "print(1 - sum(error)/error.shape[0])\n",
    "###END CODE###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression on the STFT of the data matrix\n",
    "\n",
    "Finally, let's perform logistic regression on the STFT representation of the data matrix!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use sklearn's logistic regression to classify your data after DFT encoding\n",
    "\n",
    "###BEGIN CODE###\n",
    "clf = LogisticRegression(random_state=0).fit(proj_stft, train_y_1d)\n",
    "###END CODE###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now learn a classifier on the STFT represenation using similar steps. How does the accuracy of this classifier compare to both the time domain and frequency domain classifiers? What does this tell you about the STFT representation for raw human speech?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First project the test set onto the new basis\n",
    "# Then use your classifier to get a prediction vector on your projected test set\n",
    "# Finally, compute the accuracy of your classifier on the test set\n",
    "\n",
    "###BEGIN CODE###\n",
    "test_stft = []\n",
    "for i in range(test_X.shape[0]):\n",
    "    temp = np.abs(librosa.stft(test_X[i]))\n",
    "    temp = temp[:400,:]\n",
    "    temp = temp.flatten()\n",
    "    test_stft.append(temp)\n",
    "    \n",
    "test_stft = np.vstack(test_stft)\n",
    "\n",
    "proj_test = np.dot(test_stft, new_basis_s)\n",
    "predictions = clf.predict(proj_test)\n",
    "error = test_y_1d - predictions\n",
    "error[np.abs(error) > 0] = 1\n",
    "print(1 - sum(error)/error.shape[0])\n",
    "###END CODE###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part4b\"></a>\n",
    "## Part 4b: Classification of Audio Signals with Neural Nets\n",
    "\n",
    "Now that we have attempted to classify audio signals using traditional clustering and linear classification methods, let's try using a neural network approach!  For this, we will use [Keras](https://keras.io/) to classify the time domain and STFT representations of the spoken digits dataset and compare performance to our other methods. The goal of this section is to not only show you that neural networks are powerful nonlinear classifiers for highly unstructured data, but also that careful attention to representations used when training a neural network can help improve performance! This motivates the better understanding of time and frequency domain relationships even when using deep learning methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Classification in the Time Domain\n",
    "\n",
    "First we will try to classify all 10 digits directly on the time domain representation of the signal in an end to end manner. Here you will gain some experience using deep learning libraries such as Keras to build and train neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we will construct the neural network. Go online to find the documentation for how to create a simple 1 layer\n",
    "# MLP network in Keras. The hardest part here is making sure that your dimensions match the shape of your data.\n",
    "# For this part, use Dense layers for your hidden and output layers.\n",
    "\n",
    "###BEGIN CODE###\n",
    "ip = Input(shape=(train_X[0].shape))\n",
    "hidden = Dense(128, activation='relu')(ip)\n",
    "op = Dense(10, activation='softmax')(hidden)\n",
    "#model = Model(input=ip, output=op)\n",
    "model = Model(ip, op)\n",
    "###END CODE###\n",
    "model.summary() # This will help you visualize your model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile your model here\n",
    "###BEGIN CODE###\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "###END CODE###\n",
    "\n",
    "\n",
    "# Start training your model here on the training split of the data\n",
    "\n",
    "###BEGIN CODE###\n",
    "history = model.fit(train_X,\n",
    "          train_y,\n",
    "          epochs=10,\n",
    "          batch_size=32,\n",
    "          validation_data=(test_X, test_y))\n",
    "###END CODE###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot your training and validation accuracy here\n",
    "\n",
    "###BEGIN CODE###\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "###END CODE###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Classification in the STFT Domain\n",
    "\n",
    "Now we will use the STFT representation to perform classification with a neural network. Note that, unlike the time domain representation, the STFT representation is a 2D representation which means that we can use a Convolutional Neural Network (CNN) to perform the classification. We have performed some basic preprocessing below to use our 2D data as an input to the CNN, but you will have to build and train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_ex = np.expand_dims(train_spectrograms, -1)\n",
    "test_X_ex = np.expand_dims(test_spectrograms, -1)\n",
    "print('train X shape:', train_X_ex.shape)\n",
    "print('test X shape:', test_X_ex.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the CNN architecture using Keras\n",
    "# You will have to play around with parameters to optimize performance!\n",
    "\n",
    "###BEGIN CODE###\n",
    "ip = Input(shape=train_X_ex[0].shape)\n",
    "m = Conv2D(32, kernel_size=(4, 4), activation='relu', padding='same')(ip)\n",
    "m = MaxPooling2D(pool_size=(4, 4))(m)\n",
    "m = Dropout(0.2)(m)\n",
    "m = Conv2D(64, kernel_size=(4, 4), activation='relu')(ip)\n",
    "m = MaxPooling2D(pool_size=(4, 4))(m)\n",
    "m = Dropout(0.2)(m)\n",
    "m = Flatten()(m)\n",
    "m = Dense(32, activation='relu')(m)\n",
    "op = Dense(10, activation='softmax')(m)\n",
    "\n",
    "model = Model(ip, op)\n",
    "###END CODE###\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile and train the model here\n",
    "\n",
    "###BEGIN CODE###\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_X_ex,\n",
    "          train_y,\n",
    "          epochs=10,\n",
    "          batch_size=32,\n",
    "          verbose=1,\n",
    "          validation_data=(test_X_ex, test_y))\n",
    "###END CODE###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"conclusion\"></a>\n",
    "# Part 5: Testing on Your Own Voice #\n",
    "\n",
    "Now that we have compared different methods of classifying audio signal, let's try to classify numbers using our own voice!  \n",
    "\n",
    "The first step in classifying your own voice is collecting the data.  In order to limit the variability, we can classify speech using samples taken at the same sampling rate as the data that was collected in the dataset.  In the note, you learned that by sampling at or above the nyquist rate you can perfectly reconstruct a signal.  In the code below, we start with a sampling rate of 8000 Hz.  This is sufficient to reconstruct a sample of your voice.  Try taking a sample of yourself saying one of the digits that you selected earlier in the code at a sampling rate (fs) of 8000 Hz. Feel free to play around with this rate and listen to the resulting audio signal.  Comment on what you notice.\n",
    "\n",
    "TODO: Change the sampling rate in the code below and comment on what you notice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "## Code to collect Audio sample\n",
    "## from jupyter notebook\n",
    "#################################\n",
    "\n",
    "import pyaudio\n",
    "\n",
    "\n",
    "chunk = 1024                     # Record in chunks of 1024 samples\n",
    "sample_format = pyaudio.paInt16  # 16 bits per sample\n",
    "channels = 1\n",
    "fs = 8000                        # Record at 8000 HZ to record at same sampling rate as dataset\n",
    "seconds = 2                     # Record 2 seconds of audio\n",
    "filename = \"test\"\n",
    "\n",
    "p = pyaudio.PyAudio()  # Create an interface to PortAudio\n",
    "\n",
    "print('Recording')\n",
    "\n",
    "stream = p.open(format=sample_format,\n",
    "                channels=channels,\n",
    "                rate=fs,\n",
    "                frames_per_buffer=chunk,\n",
    "                input=True)\n",
    "\n",
    "frames = []  # Initialize array to store frames\n",
    "\n",
    "for i in range(0, int(fs / chunk * seconds)):\n",
    "    data = stream.read(chunk)\n",
    "    frames.append(data)\n",
    "    \n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "p.terminate()\n",
    "\n",
    "print('Finished recording')\n",
    "\n",
    "\n",
    "wf = wave.open(filename +\".wav\", 'wb')\n",
    "wf.setnchannels(channels)\n",
    "wf.setsampwidth(p.get_sample_size(sample_format))\n",
    "wf.setframerate(fs)\n",
    "wf.writeframes(b''.join(frames))\n",
    "wf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(\"test.wav\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict on Your Own Voice\n",
    "\n",
    "Before you run the following cell, how do you think the model will classify your speech?  What characteristics about your voice do you think could influence the classfication of the speech?  Now run the cell.  Was it able to classify your speech correctly?  If it did not, what are some reasons why this model might have had a hard time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav, sr = librosa.load(\"test.wav\")\n",
    "\n",
    "pad2d = lambda a, i: a[:, 0: i] if a.shape[1] > i else np.hstack((a, np.zeros((a.shape[0],i - a.shape[1]))))\n",
    "spectogram = np.abs(librosa.stft(wav))\n",
    "padded_spectogram = pad2d(spectogram, 40)\n",
    "# print(padded_spectogram.shape)\n",
    "temp = test_X_ex[0]\n",
    "\n",
    "test_spectograms = np.array(padded_spectogram)\n",
    "test_spectograms = np.expand_dims(test_spectograms, -1)\n",
    "test_spectograms = test_spectograms[np.newaxis, :, :]\n",
    "# print(test_X_ex.shape)\n",
    "prediction = model.predict(test_spectograms)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!\n",
    "\n",
    "Great job! You have completed this lab.  Hopefully you now appreciate the power of the frequency domain and Fourier Transform and their many applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Allen V. Oppenheim, Signals and Systems, Second Edition, 1997\n",
    "\n",
    "Berkeley Microscopy, Capturing images\n",
    "http://microscopy.berkeley.edu/courses/dib/sections/02Images/sampling.html\n",
    "\n",
    "Jarno Seppänen, Audio Signal Processing basics, 1999\n",
    "https://www.cs.tut.fi/sgn/arg/intro/basics.html\n",
    "\n",
    "Dima Shulga, Speech Classification Using Neural Networks: https://towardsdatascience.com/speech-classification-using-neural-networks-the-basics-e5b08d6928b7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
